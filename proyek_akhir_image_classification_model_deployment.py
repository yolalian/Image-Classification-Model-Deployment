# -*- coding: utf-8 -*-
"""Proyek_Akhir : Image Classification Model Deployment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VN7LbtFTD2cx7MuWCcFXgGphzmJ_fUhr

Nama : Yolanda Ester Berliana Ritonga

Email : yolandaesterbrtg@gmail.com

Proyek Akhir : Image Classification Model Deployment
"""

!pip install opendatasets

import opendatasets as od
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
import shutil
import tensorflow as tf

"""**Download Dataset**"""

od.download('https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset')

dataset_path = '/content/rice-image-dataset/Rice_Image_Dataset'

"""**Menghapus kelas .ipynb_checkpoints**"""

if '.ipynb_checkpoints' in os.listdir(dataset_path):
    os.rmdir(os.path.join(dataset_path, '.ipynb_checkpoints'))

class_counts = {}
total_samples = 0

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_path):
        num_samples = len(os.listdir(class_path))
        class_counts[class_name] = num_samples

for class_name, count in class_counts.items():
    print(f"Kelas: {class_name}, Jumlah Sampel: {count}")

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)

    if os.path.isdir(class_path):
        num_samples = len(os.listdir(class_path))
        total_samples += num_samples

print(f"Total Keseluruhan Sampel: {total_samples}")

num_classes = len(class_counts)
print(f"Jumlah Kelas: {num_classes}")

"""**Membuat direktori baru untuk train dan test**"""

train_dir = '/content/rice-image-dataset/train'
test_dir = '/content/rice-image-dataset/test'

"""**Membagi dataset menjadi train dan test**"""

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)

    if os.path.isdir(class_path):
        image_list = os.listdir(class_path)
        train_images, test_images = train_test_split(image_list, test_size=0.2, random_state=42)

        # Membuat direktori kelas di train dan test
        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

        # Menyalin gambar ke direktori train
        for image in train_images:
            shutil.copy(os.path.join(class_path, image), os.path.join(train_dir, class_name, image))

        # Menyalin gambar ke direktori test
        for image in test_images:
            shutil.copy(os.path.join(class_path, image), os.path.join(test_dir, class_name, image))

"""**Image Data Augmentation**"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(rescale=1./255)

"""**Path untuk train dan test**"""

train_data_path = '/content/rice-image-dataset/train'
test_data_path = '/content/rice-image-dataset/test'

"""**Data dari Directory**"""

train_generator = train_datagen.flow_from_directory(
    train_data_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_data_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

"""**Membangun Model**"""

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in base_model.layers:
    layer.trainable = False

model = Sequential()
model.add(base_model)
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_generator.class_indices), activation='softmax'))

"""**Compile Model**"""

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""**Penggunaan Callbacks**"""

checkpoint = ModelCheckpoint("best_model.h5", save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

"""**Melatih Model**"""

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // 32,
    epochs=20,
    validation_data=test_generator,
    validation_steps=test_generator.samples // 32,
    callbacks=[checkpoint, early_stopping]
)

"""**Membuat Akurasi untuk Training dan Validation**"""

train_accuracy = history.history['accuracy'][-1] * 100
validation_accuracy = history.history['val_accuracy'][-1] * 100

print(f'Akurasi Training: {train_accuracy:.2f}%')
print(f'Akurasi Validation: {validation_accuracy:.2f}%')

"""**Membuat Plot Akurasi**"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy (%)')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**Membuat Plot Loss**"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**Menyimpan model ke dalam format TF-Lite**"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

file_size = os.path.getsize('model.tflite')
print(f"Ukuran File TFLite: {file_size} bytes")